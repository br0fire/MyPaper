
# Обзор статей

|№| Название      | Авторы        |Год | Ссылка |Краткое описание| 
|-| ------------- | ------------- |----|---|--------|
|1| Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets  | Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra |  2022 | [link](https://arxiv.org/abs/2201.02177)  |  Самая первая статья про гроккинг.     |
|2| On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima  | Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang |2017|[link](https://arxiv.org/abs/1609.04836)| Кривизна поверхности функции потерь скоррелирована с обобщающей способностью.|
|3| Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs  | Timur Garipov and Pavel Izmailov and Dmitrii Podoprikhin and Dmitry Vetrov and Andrew Gordon Wilson| 2018  |  [link](https://arxiv.org/abs/1802.10026)  | Основная статья про соедининие локальных потимумов с помощью простых кривых.|
|4| Omnigrok: Grokking Beyond Algorithmic Data  | Ziming Liu and Eric J. Michaud and Max Tegmark  |  2023     | [link](https://arxiv.org/abs/2210.01117)    | Наблюдение эффекта гроккинга на алгоритмических задачах, а также на MNIST  |
|5| Deep Double Descent: Where Bigger Models and More Data Hurt  | Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever  | 2019      | [link](https://arxiv.org/abs/1912.02292)       | Двойной спуск в нейронных сетях: по мере увеличения размера модели качество сначала ухудшается, а затем улучшается.|

